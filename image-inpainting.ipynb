{"cells":[{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np\n","import os\n","import glob\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torchvision.models import vgg19\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision.utils import save_image, make_grid\n","\n","\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from tqdm import tqdm_notebook as tqdm\n","\n","random.seed(42)\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# number of epochs of training\n","n_epochs = 75\n","# size of the batches\n","batch_size = 16\n","# name of the dataset\n","dataset_name = \"../input/celeba-dataset/img_align_celeba/img_align_celeba\"\n","# adam: learning rate\n","lr = 0.00008\n","# adam: decay of first order momentum of gradient\n","b1 = 0.5\n","# adam: decay of first order momentum of gradient\n","b2 = 0.999\n","# number of cpu threads to use during batch generation\n","n_cpu = 4\n","# size of each image dimension\n","img_size = 128\n","# size of random mask\n","mask_size = 64\n","# number of image channels\n","channels = 3\n","# interval between image sampling\n","sample_interval = 5000\n","#adversarial loss ratio\n","Lambda=0.001\n","\n","cuda = True if torch.cuda.is_available() else False\n","# os.makedirs(\"testImages\", exist_ok=True)\n","# os.makedirs(\"trainImages\", exist_ok=True)\n","# os.makedirs(\"saved_models\", exist_ok=True)\n","\n","# Calculate output dims of image discriminator (PatchGAN)\n","patch_h, patch_w = int(img_size / 2 ** 3), int(img_size / 2 ** 3)\n","patch = (1, patch_h, patch_w)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["class ImageDataset(Dataset):\n","    def __init__(self, root, transforms_=None, img_size=128, mask_size=64, mode=\"train\"):\n","        self.transform = transforms.Compose(transforms_)\n","        self.img_size = img_size\n","        self.mask_size = mask_size\n","        self.mode = mode\n","        self.files = sorted(glob.glob(\"%s/*.jpg\" % root))\n","        self.files = self.files[:-4000] if mode == \"train\" else self.files[-4000:]\n","\n","    def apply_random_mask(self, img):\n","        \"\"\"Randomly masks image\"\"\"\n","        y1, x1 = np.random.randint(0, self.img_size - self.mask_size, 2)\n","        y2, x2 = y1 + self.mask_size, x1 + self.mask_size\n","        masked_part = img[:, y1:y2, x1:x2]\n","        masked_img = img.clone()\n","        masked_img[:, y1:y2, x1:x2] = 0\n","\n","        return masked_img\n","\n","    def __getitem__(self, index):\n","        img = Image.open(self.files[index % len(self.files)])\n","        img = self.transform(img)\n","        masked_img= self.apply_random_mask(img)\n","\n","        return img, masked_img\n","\n","    def __len__(self):\n","        return len(self.files)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# transforms_ = [\n","#     transforms.Resize((img_size, img_size), Image.BICUBIC),\n","#     transforms.ToTensor(),\n","#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","# ]\n","# dataloader = DataLoader(\n","#     ImageDataset(dataset_name, transforms_=transforms_),\n","#     batch_size=batch_size,\n","#     shuffle=True,\n","#     num_workers=n_cpu,\n","# )\n","# test_dataloader = DataLoader(\n","#     ImageDataset(dataset_name, transforms_=transforms_, mode=\"test\"),\n","#     batch_size=batch_size,\n","#     shuffle=True,\n","#     num_workers=n_cpu,\n","# )"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["class Generator(nn.Module):\n","    def __init__(self, channels=3):\n","        super(Generator, self).__init__()\n","\n","        def downsample(in_feat, out_feat, normalize=True):\n","            layers = [nn.Conv2d(in_feat, out_feat, 4, stride=2, padding=1)]\n","            if normalize:\n","                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n","            layers.append(nn.LeakyReLU(0.2))\n","            return layers\n","\n","        def upsample(in_feat, out_feat, normalize=True):\n","            layers = [nn.ConvTranspose2d(in_feat, out_feat, 4, stride=2, padding=1)]\n","            if normalize:\n","                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n","            layers.append(nn.ReLU())\n","            return layers\n","\n","        self.model = nn.Sequential(\n","            *downsample(channels, 64, normalize=False),\n","            *downsample(64, 64),\n","            *downsample(64, 128),\n","            *downsample(128, 256),\n","            *downsample(256, 512),\n","            nn.Conv2d(512, 4000, 1),\n","            *upsample(4000, 512),\n","            *upsample(512, 256),\n","            *upsample(256, 128),\n","            *upsample(128,64),\n","            *upsample(64,32),\n","            nn.Conv2d(32, channels, 3, 1, 1),\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, channels=3):\n","        super(Discriminator, self).__init__()\n","\n","        def discriminator_block(in_filters, out_filters, stride, normalize):\n","            \"\"\"Returns layers of each discriminator block\"\"\"\n","            layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n","            if normalize:\n","                layers.append(nn.InstanceNorm2d(out_filters))\n","            layers.append(nn.LeakyReLU(0.2, inplace=True))\n","            return layers\n","\n","        layers = []\n","        in_filters = channels\n","        for out_filters, stride, normalize in [(128, 2, False), (256, 2, True), (512, 2, True), (1024, 1, True)]:\n","            layers.extend(discriminator_block(in_filters, out_filters, stride, normalize))\n","            in_filters = out_filters\n","\n","        layers.append(nn.Conv2d(out_filters, 1, 3, 1, 1))\n","\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, img):\n","        return self.model(img)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([16, 3, 128, 128])"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["image = torch.randn(16,3,128,128)\n","gen = Generator(channels = 3)\n","gen_output = gen(image)\n","image.shape"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([16, 1, 16, 16])"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["disc = Discriminator(channels = 3)\n","disc_out = disc(image)\n","disc_out.shape"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def weights_init_normal(m):\n","    classname = m.__class__.__name__\n","    if classname.find(\"Conv\") != -1:\n","        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n","    elif classname.find(\"BatchNorm2d\") != -1:\n","        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        torch.nn.init.constant_(m.bias.data, 0.0)\n","\n","def save_sample_test(batches_done):\n","    samples, masked_samples = next(iter(test_dataloader))\n","    samples = Variable(samples.type(Tensor))\n","    masked_samples = Variable(masked_samples.type(Tensor))\n","    # Generate inpainted image\n","    gen_img = generator(masked_samples)\n","    # Save sample\n","    sample = torch.cat((samples.data ,masked_samples.data, gen_img.data), -2)\n","    save_image(sample, \"testImages/%d.png\" % batches_done, nrow=4, normalize=True)\n","\n","def save_sample_train(batches_done):\n","    samples, masked_samples = next(iter(dataloader))\n","    samples = Variable(samples.type(Tensor))\n","    masked_samples = Variable(masked_samples.type(Tensor))\n","    # Generate inpainted image\n","    gen_img = generator(masked_samples)\n","    # Save sample\n","    sample = torch.cat((samples.data ,masked_samples.data, gen_img.data), -2)\n","    save_image(sample, \"trainImages/%d.png\" % batches_done, nrow=4, normalize=True)\n","    \n","# Loss function\n","adversarial_loss = torch.nn.MSELoss()\n","pixelwise_loss = torch.nn.L1Loss()\n","\n","# Initialize generator and discriminator\n","generator = Generator(channels=channels)\n","discriminator = Discriminator(channels=channels)\n","\n","# Initialize weights\n","generator.apply(weights_init_normal)\n","discriminator.apply(weights_init_normal)\n","\n","if cuda:\n","    generator.cuda()\n","    discriminator.cuda()\n","    adversarial_loss.cuda()\n","    pixelwise_loss.cuda()\n","\n","# Optimizers\n","optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n","optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n","\n","Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch :0\n"]},{"ename":"NameError","evalue":"name 'dataloader' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch :\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m gen_adv_loss, gen_pixel_loss, disc_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 5\u001b[0m tqdm_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[43mdataloader\u001b[49m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataloader)))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (imgs, masked_imgs) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm_bar):\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Adversarial ground truths\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     valid \u001b[38;5;241m=\u001b[39m Variable(Tensor(imgs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39mpatch)\u001b[38;5;241m.\u001b[39mfill_(\u001b[38;5;241m1.0\u001b[39m), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","\u001b[1;31mNameError\u001b[0m: name 'dataloader' is not defined"]}],"source":["for epoch in range(n_epochs):\n","    print(f'epoch :{epoch}')\n","    \n","    gen_adv_loss, gen_pixel_loss, disc_loss = 0, 0, 0\n","    tqdm_bar = tqdm(dataloader, desc=f'Training Epoch {epoch} ', total=int(len(dataloader)))\n","    for i, (imgs, masked_imgs) in enumerate(tqdm_bar):\n","\n","        # Adversarial ground truths\n","        valid = Variable(Tensor(imgs.shape[0], *patch).fill_(1.0), requires_grad=False)\n","        fake = Variable(Tensor(imgs.shape[0], *patch).fill_(0.0), requires_grad=False)\n","\n","        # Configure input\n","        imgs = Variable(imgs.type(Tensor))\n","        masked_imgs = Variable(masked_imgs.type(Tensor))\n","\n","        ## Train Generator ##\n","        with torch.cuda.amp.autocast():\n","            # Generate a batch of images\n","            gen = generator(masked_imgs)\n","\n","            # Adversarial and pixelwise loss\n","            g_adv = adversarial_loss(discriminator(gen), valid)\n","            g_pixel = pixelwise_loss(gen, imgs)\n","            # Total loss\n","            g_loss = Lambda * g_adv + (1-Lambda) * g_pixel\n","            \n","        optimizer_G.zero_grad()\n","        g_loss.backward()\n","        optimizer_G.step()\n","\n","        ## Train Discriminator ##\n","        with torch.cuda.amp.autocast():\n","            # Measure discriminator's ability to classify real from generated samples\n","            real_loss = adversarial_loss(discriminator(imgs), valid)\n","            fake_loss = adversarial_loss(discriminator(gen.detach()), fake)\n","            d_loss = 0.5 * (real_loss + fake_loss)\n","\n","        optimizer_D.zero_grad()\n","        d_loss.backward()\n","        optimizer_D.step()\n","        \n","        gen_adv_loss += g_adv.item()\n","        gen_pixel_loss += g_pixel.item()\n","        disc_loss += d_loss.item()\n","        tqdm_bar.set_postfix(gen_adv_loss=gen_adv_loss/(i+1), gen_pixel_loss=gen_pixel_loss/(i+1), disc_loss=disc_loss/(i+1))\n","        \n","        \n","        # Generate sample at sample interval\n","        generator.eval()\n","        with torch.inference_mode():\n","            batches_done = epoch * len(dataloader) + i\n","            if batches_done % sample_interval == 0:\n","                save_sample_test(batches_done)\n","                save_sample_train(batches_done)\n","                \n","    print(f'gen_adv_loss={gen_adv_loss/(i+1)}, gen_pixel_loss={gen_pixel_loss/(i+1)}, disc_loss={disc_loss/(i+1)}')\n","    torch.save(generator.state_dict(), \"/kaggle/working/saved_models/generator.pth\")\n","    torch.save(discriminator.state_dict(), \"/kaggle/working/saved_models/discriminator.pth\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":29561,"sourceId":37705,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
